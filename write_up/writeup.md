
# Vehicle Detection Project

The goals / steps of this project are the following:

* Training a Support Vector Machine classifier
* Process camera images and classify cars in the video stream.

[//]: # (Image References)
[image1]: ./car.png
[image2]: ./non-car.jpg
[image3]: ./test4-detect.png
[image4]: ./test4-detect0.png
[image5]: ./grid.png

[image6]: ./hog-chan00-detect.png
[image7]: ./hog-chan11-detect.png
[image8]: ./hog-chan22-detect.png
[image9]: ./hog-chanAll3-detect.png
[image10]: ./hog-input.jpg

 [video1]: ./project_video.mp4
[video2]: ./project_video_intermediate.mp4
[video3]: ./project_video_output.mp4




## Training Data Set

The project requires a wide range of labelled data with both cars and non car images from road scenes. The project used data from GTI and KITTI data sets. The program flow was debugged using a small subset of the data but transitioned to the full data set once the pipeline was debugged.

The following is an example of a car image from the data set ![alt text][image1]

Example non-car image from the data set.  ![alt text][image2]


The project builds up a feature vector from the images used in the training set.

The data set is loaded and trained in the train.py file
The training  set looked to be relatively balanced between car and non car images in the data set.
* Car images: 8792
* Non Car images: 8968

## Training Feature sets selected

The Features used to train the classifier were
* Histogram of Gradients
* Colour Histogram
* Spatial Features

The concatenated feature vector was 8460 in length.

The feature extraction code resides in features.py

## Colour space
 The colour space was tested using RGB, LHS and YCrCb. Imperially the best outcomes from the entire pipeline were generated by using the YCrCb colour space.

## Histogram of Gradients

The HOG featured uses the hog function from the skimage library.
The key parameters used were numeral of orientations pixels per cell, cells per block.
```python

orientations = 9  # HOG orientations
pix_per_cell = (8,8) # HOG pixels per cell
cell_per_block = (2,2) # HOG cells per block
```
These were chosen via trial and error with inspection of intermediate output images from hog visualisation.

```python
from skimage.feature import hog

features = hog(img, orientations=orient,
               pixels_per_cell=(pix_per_cell, pix_per_cell),
               cells_per_block=(cell_per_block, cell_per_block),
               transform_sqrt=True,
               visualise=vis, feature_vector=feature_vec)
```

Historgram used 32 bins.



Here is an example test image used for the HOG processing. The image was processed in YcbCr colour space.
https://en.wikipedia.org/wiki/YCbCr

![alt text][image10]

To get a visualisation image of the 1 component of the image :
```python
Features , hog_image_1 = get_hog_features(feature_image[:,:,0], orient, pix_per_cell, cell_per_block,
                        vis=True, feature_vec=True)
scipy.misc.imsave('saved_hog.png'',hog_image_1)
```

The Y component generated the following HOG  
![alt text][image6]
The Cb component generated the following HOG  

![alt text][image7]
The Cr component generated the following HOG  
![alt text][image8]

Merging the 3 HOG greyscale images into a colour image produces the following:
![alt text][image9]
This is the HOG feature vector that was used for training and classification.


## Colour Histogram
A histogram of the colour space was created using all 3 channels of the target colour space. The histograms were concatenated to create the colour features.

```python
channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)
channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)
channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)
# Concatenate the histograms into a single feature vector
hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))
```


## Spatial Features

 The feature vector is generated using a simple open cv resize feature. The ravel call flattens the resized image to a feature vector.
 ```python
 features = cv2.resize(img, size).ravel()
 ```

## Overall tuning/variables

The code we developed to allow for a range of options to be trailed.
The following variables are defined for both the training.py and pipeline.py files.

```python
colorspace = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb
orient = 9  # HOG orientations
pix_per_cell = 8 # HOG pixels per cell
cell_per_block = 2 # HOG cells per block
hog_channel = 'ALL' # Can be 0, 1, 2, or "ALL"
spatial=32
spatial_size = (spatial, spatial) # Spatial binning dimensions
hist_bins = 32    # Number of histogram bins
spatial_feat = True # Spatial features on or off
hist_feat = True # Histogram features on or off
hog_feat = True # HOG features on or off
y_start_stop = [400, 650] # Min and max in y to search in slide_window()
```

## SVG training

Vectors were created for all car and non car images. A labelled data set was used to train the Support Vector Machine.

The data set was split randomly into a training and validation test set
```python
X_train, X_test, y_train, y_test = train_test_split(
    scaled_X, y, test_size=0.2, random_state=rand_state)
```
The classifier was created and trained with C = 1.0
```python
svc = LinearSVC(C=1.0)
clf = svc
clf.fit(X_train, y_train)
```
The classifier accuracy was verified with the test data    
```python
print('Test Accuracy of SVC = ', round(clf.score(X_test, y_test), 4))
```

The result of the training was as follows

```python
7.21 Seconds to train SVC...
Test Accuracy of SVC =  0.9924
My SVC predicts:      [ 0.  1.  1.  1.  0.  1.  1.  1.  0.  0.]
For these 10 labels:  [ 0.  1.  1.  1.  0.  1.  1.  1.  0.  0.]
0.00135 Seconds to predict 10 labels with SVC
saving model to classifier-svm.pkl
```

# Processing pipeline

The video processing pipeline is in pipeline.py

The flow of processing is as follows
* Create a series of search boxes with different scales to run across the image. The principle was to use larger boxes at the bottom of the image where the images are closer, and smaller search images for the further away regions as the search items would be smaller.  The image search space was limited to the bottom third of the images.
* Search each box to see if it contains a car or not.
* For each box, calculate a heatmap of pixels included by the box if a car was detected.
* Apply a threshold to remove spurious boxes. (eg only a single box in an area)
* For video - process a number of frames, collecting candidate boxes from multiple frames.
* create a heatmap across multiple frames.
* Select the bounding boxes for the heatmap boxes that pass a threshold.
* Display the boxes on the output video.


## Window sweeps
```python

windows = slide_window(image, x_start_stop=[None, None], y_start_stop=[y_start_stop[0], y_start_stop[1]],
            xy_window=(256, 256), xy_overlap=(0.8, 0.8))

windows = windows + slide_window(image, x_start_stop=[None, None], y_start_stop=[y_start_stop[0], 600],
            xy_window=(128, 128), xy_overlap=(0.8, 0.8))


windows = windows + slide_window(image, x_start_stop=[300, 1000], y_start_stop=[y_start_stop[0], 480],
            xy_window=(64, 64), xy_overlap=(0.9, 0.9))
```

The example window created is shown below
![alt text][image5]

## Searching for car

file: window.py


There search windows are created above, for each window the main image is cropped and scaled to that window. A prediction from the classifier is then requested. If it matches it adds the window to an output list.

```python
test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64))
#4) Extract features for that window using single_img_features()
features = single_img_features(test_img, color_space=color_space,
                    spatial_size=spatial_size, hist_bins=hist_bins,
                    orient=orient, pix_per_cell=pix_per_cell,
                    cell_per_block=cell_per_block,
                    hog_channel=hog_channel, spatial_feat=spatial_feat,
                    hist_feat=hist_feat, hog_feat=hog_feat)
#5) Scale extracted features to be fed to classifier
test_features = scaler.transform(np.array(features).reshape(1, -1))
#6) Predict using your classifier
prediction = clf.predict(test_features)
#7) If positive (prediction == 1) then save the window
if prediction == 1:
    on_windows.append(window)

```

# Spurious rejection

For each box, calculate a heatmap of pixels included by the box if a car was detected. Apply a threshold to remove spurious boxes. (eg only a single box in an area)

This image shows all boxes predicated by the classifier in yellow
![alt text][image3]

The threshold of the heatmap was applied to create the boxes in red showing the bounding box of the heatmap

 From the positive detections I created a heatmap and then thresholded that map to identify vehicle positions.  I then used `scipy.ndimage.measurements.label()` to identify individual blobs in the heatmap.  I then assumed each blob corresponded to a vehicle.  I constructed bounding boxes to cover the area of each blob detected. The image below shows the bounding box from the heat map in red.

![alt text][image4]

Note: The images shown  are in the target colour space.


The output vide processed is : ![alt text][video_3]

###Discussion

Some end of project comments

* The real time performance (using python) is very poor.
* The identification of the best colour space took quite some time, It would be useful to have general colour space and hog processing pipelines for visual inspection and code development (a visual tool)
The pipeline is generated with considerable empirical testing. It is likely to be sensitive to lighting conditions.
* It would be good to investigate the relative contribution of colour histogram, special and HOG are contributing to the overall match to reduce the feature vector size.
* The search windows are too dense, I think I could tune to remove some of the smaller windows.






### END
